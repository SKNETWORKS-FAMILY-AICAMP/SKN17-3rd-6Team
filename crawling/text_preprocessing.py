import os, json, re, unicodedata, glob
import pdfplumber
from collections import OrderedDict
import warnings
import logging

warnings.filterwarnings("ignore", message="Could get FontBBox")
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# ---------- í…ìŠ¤íŠ¸ ì •ë¦¬ ----------
def norm(s: str) -> str:
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", str(s))
    s = s.replace("\xa0", " ")
    s = re.sub(r"[\r\n\t]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip(" -:Â·|")
    return s.strip()

def key_id(s: str) -> str:
    return re.sub(r"[^0-9a-zê°€-í£]+", "", norm(s).lower())

def dedup_keep_order(seq):
    seen = set()
    out = []
    for x in seq:
        k = key_id(x)
        if k and k not in seen:
            seen.add(k)
            out.append(norm(x))
    return out

# ---------- í‘œ íŒŒì„œ ("í˜„ìƒ | ì˜ˆìƒ ì›ì¸" ì–‘ì‹ë§Œ) ----------
def parse_table_target(page):
    result = OrderedDict() # ë¹„ì–´ìˆëŠ” ì•„ì´
    tables = page.extract_tables() or []

    for table in tables:
        last_ph = None
        last_cause = None

        if not table or len(table[0]) < 2:
            # print('ccc')
            continue

        header = [norm(x) for x in table[0]]
        #print('ğŸˆğŸˆ', header)
        a = 0
        header_split = []
        found_indices = {}

        for idx, component in enumerate(header) :
            splitted = component.split(" ")
            header_split.extend(splitted)

            if "í˜„ìƒ" in splitted:
                found_indices["í˜„ìƒ"] = idx
            if "ì½”ë“œ" in splitted:
                found_indices["ì½”ë“œ"] = idx
            if "ì›ì¸" in splitted:
                found_indices["ì›ì¸"] = idx

            # í˜„ëŒ€ ì „ì²˜ë¦¬ 
            #if "í˜„ìƒ" in splitted:
            #    found_indices["í˜„ìƒ"] = idx
            #if "ì½”ë“œ" in splitted:
            #    found_indices["ì½”ë“œ"] = idx
            #if "ì›ì¸" in splitted:
            #    found_indices["ì›ì¸"] = idx
            #if 'í˜„' in splitted:
            #    found_indices["í˜„"] = idx
            #if 'ì¦ìƒ' in splitted:
            #    found_indices["ì¦ìƒ"] = idx
            #if 'ìœ í˜•' in splitted:
            #    found_indices["ìœ í˜•"] = idx
            #if 'ê³ ì¥í˜„ìƒ' in splitted:
            #    found_indices["ê³ ì¥í˜„ìƒ"] = idx
            #if 'ì ê²€í•­ëª©' in splitted:
            #    found_indices["ì ê²€í•­ëª©"] = idx
            #if 'ê³ ì¥' in splitted:
            #    found_indices["ê³ ì¥"] = idx

        if (("í˜„ìƒ" in found_indices or "ì½”ë“œ" in found_indices) and "ì›ì¸" in found_indices):
          
           # í•œ í˜„ìƒì´ ì—¬ëŸ¬ ì›ì¸ì„ ê°€ì§€ëŠ” ê²½ìš° - í•©ì¹˜ê¸°
            for row in table[1:]:
                # print('row : ', row)
                if not row or len(row) < 2:   # í•´ë‹¹ì—†ìŒ
                    # print('aaa')
                    continue
                idx_ph = found_indices.get("í˜„ìƒ", found_indices.get("ì½”ë“œ"))
                idx_cause = found_indices["ì›ì¸"]

            # í•œ ì›ì¸ì´ ì—¬ëŸ¬ ê²°ê³¼ë¥¼ ê°€ì§€ëŠ” ê²½ìš°ë„ í•©ì¹˜ê¸°
                # ph = norm(row[idx_ph]) if idx_ph is not None and idx_ph < len(row) else None
                # cause = norm(row[idx_cause]) if idx_cause < len(row) else None

                ph_cell = row[idx_ph] if idx_ph is not None and idx_ph < len(row) else ""
                ph = norm(ph_cell).strip() if ph_cell else None
                if ph:
                    last_ph = ph
                else:
                    ph = last_ph  # ë¹ˆ ê°’ì´ë©´ ì´ì „ í˜„ìƒ ì‚¬ìš©

                # ì›ì¸ ê°€ì ¸ì˜¤ê¸°
                cause_cell = row[idx_cause] if idx_cause is not None and idx_cause < len(row) else ""
                cause = norm(cause_cell).strip() if cause_cell else None
                if cause:
                    last_cause = cause
                else:
                    cause = last_cause  # ë¹ˆ ê°’ì´ë©´ ì´ì „ ì›ì¸ ì‚¬ìš©

                if ph and cause:
                    result.setdefault(ph, [])
                    if cause not in result[ph]:
                        result[ph].append(cause)
    return result

# ---------- í‘œ ìœ„ í…ìŠ¤íŠ¸(í˜„ìƒ ì œëª©) + í‘œ ë§¤ì¹­ ----------
def parse_table_with_title(page):
    result = OrderedDict()

    # ëª¨ë“  í…ìŠ¤íŠ¸ ì¢Œí‘œ ì¶”ì¶œ
    words = page.extract_words()
    lines = {}
    for w in words:
        y = round(w["top"])
        lines.setdefault(y, []).append(w["text"])
    sorted_lines = sorted(lines.items(), key=lambda x: x[0])

    # í‘œ ì°¾ê¸°
    tables = page.find_tables()
    for table in tables:
        top_y = table.bbox[1]
        # í‘œ ìœ„ìª½ ê°€ì¥ ê°€ê¹Œìš´ ì¤„ ì°¾ê¸°
        candidates = [line for y, line in sorted_lines if y < top_y]
        if not candidates:
            continue
        title = " ".join(candidates[-1])  # ê°€ì¥ ê°€ê¹Œìš´ ì¤„

        # í‘œ ë°ì´í„° ì¶”ì¶œ
        # data = table.extract()
        rows = table.extract() 
        causes = []
        header = [norm(x) for x in rows[0]]
        # print('ğŸˆğŸˆ', header)
        header_split = []
        found_indices = {}
        last_ph = []
        last_cause = []

        for idx, component in enumerate(header) :
            splitted = component.split(" ")
            header_split.extend(splitted)

            if "ì›ì¸" in splitted:
                found_indices["ì›ì¸"] = idx
                # print(idx)

        if "ì›ì¸" in found_indices:
           # í•œ í˜„ìƒì´ ì—¬ëŸ¬ ì›ì¸ì„ ê°€ì§€ëŠ” ê²½ìš° - í•©ì¹˜ê¸°
            for row in rows[1:]:
                if not row or len(row) < 2:   
                    continue
            
            # í•œ ì›ì¸ì´ ì—¬ëŸ¬ ê²°ê³¼ë¥¼ ê°€ì§€ëŠ” ê²½ìš°
                idx_cause = found_indices["ì›ì¸"]
                
                ph_cell = row[idx_cause] if idx_cause is not None and idx_cause < len(row) else ""
                ph = norm(ph_cell).strip() if ph_cell else None

                if ph:
                    last_ph = ph
                else:
                    ph = last_ph 
                
                # ì›ì¸ ê°€ì ¸ì˜¤ê¸°
                cause_cell = row[idx_cause] if idx_cause is not None and idx_cause < len(row) else ""
                cause = norm(cause_cell).strip() if cause_cell else None
                if cause:
                    last_cause = cause
                else:
                    cause = last_cause  # ë¹ˆ ê°’ì´ë©´ ì´ì „ ì›ì¸ ì‚¬ìš©

                if ph and cause:
                    result.setdefault(title, [])
                    if cause not in result[title]:
                        result[title].append(cause)

    return result


# ---------------- ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ------------------- 
def transform_categories(parts):
    # cats = parts[3:]  # ì• 3ê°œ ë¬´ì‹œ (ev3~ev6)
    cats = parts
    cats = dedup_keep_order(cats)
    if len(cats) == 2:
        return [cats[0], "ì—†ìŒ", "ê³ ì¥ì§„ë‹¨"]
    elif len(cats) == 3:
        return [cats[0], cats[1], "ê³ ì¥ì§„ë‹¨"]
    elif len(cats) >= 4:
        return [cats[0], cats[1] + "+" + cats[2], "ê³ ì¥ì§„ë‹¨"]
    else:
        return []


# ----------------- FaulMap ë³‘í•© ---------------------
def merge_faultmaps(dst: dict, src: dict):
    for ph, causes in src.items():
        dst.setdefault(ph, [])
        for c in causes:
            if c not in dst[ph]:
                dst[ph].append(c)


# ---------------- PDF í•œê°œ ì²˜ë¦¬ -----------------------
def parse_pdf(path):
    parent_folder = os.path.basename(os.path.dirname(os.path.abspath(path))) or "ROOT"
    result = {parent_folder: OrderedDict()}
    found_valid = False

    # with pdfplumber.open(path) as pdf:
    # with fitz.open(path) as pdf:
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            # header lineì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            text = page.extract_text() or ''
            header_line = None
            for line in text.split("\n"):
                if ">" in line:
                    header_line = line
                    break
            if not header_line:
                continue  # > ì—†ëŠ” PDFëŠ” skip

            parts = [norm(p) for p in header_line.split(">")]
            cats = transform_categories(parts)
            concats = '+'.join(cats)

            if not cats:
                continue

            # # í‘œ ë°ì´í„° ì¶”ì¶œ
            # table_data = parse_table_target(page)
            # if table_data:
            #     # print('result[parent_folder] : ', result[parent_folder])
            #     print(parent_folder)
            #     print('cats : ', cats)
            #     result[parent_folder].setdefault(concats, {}).update(table_data)
            #     found_valid = True

            # ë‘ ë°©ì‹ ë³‘í•©
            table_data1 = parse_table_target(page)       # í‘œ ì•ˆì— í˜„ìƒì´ ìˆëŠ” ê²½ìš°
            table_data2 = parse_table_with_title(page)   # í‘œ ìœ„ ì œëª©ì´ ìˆëŠ” ê²½ìš°
            table_data = {}
            merge_faultmaps(table_data, table_data1)
            merge_faultmaps(table_data, table_data2)

            if table_data:
                result[parent_folder].setdefault(concats, {}).update(table_data)
                found_valid = True


    return result if found_valid else None
    # return table_data if found_valid else None

# ---------- JSON íŒŒì¼ ê¸°í˜¸ ì²˜ë¦¬ --------
def clean_dict(obj):
    if isinstance(obj, dict):
        return {k: clean_dict(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_dict(x) for x in obj]
    elif isinstance(obj, str):
        # ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±°
        return obj.replace("â€“", "").replace("â€¢", "").strip()
    else:
        return obj

# ---------- ì—¬ëŸ¬ PDF ì²˜ë¦¬ ----------
def parse_pdfs(folder_path, output_json):
    final_result = OrderedDict()
    pdf_files = glob.glob(os.path.join(folder_path, "**/*.pdf"), recursive=True)
    # print(len(pdf_files))

    for pdf_file in pdf_files:
        data = parse_pdf(pdf_file)
        # print(data)
        if not data:  # ëŒ€ìƒ ì–‘ì‹ ì•„ë‹ˆë©´ skip
            continue

        for folder, content in data.items():
            if folder not in final_result:
                final_result[folder] = content
            else:
                for k, v in content.items():
                    final_result[folder].setdefault(k, {}).update(v)

    cleaned_result = clean_dict(final_result)

    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(cleaned_result, f, ensure_ascii=False, indent=2)

    print("ì´ PDF ê°œìˆ˜:", len(pdf_files))
    print("ëŒ€ìƒ ì–‘ì‹ ì¶”ì¶œ ê°œìˆ˜:", sum(1 for pdf in pdf_files if parse_pdf(pdf)))
    print("ì €ì¥ ì™„ë£Œ:", output_json)

#---------- json í´ë”ëª… ë°˜í™˜ -------------

folder_path = "./kia_data"
subfolders = [name for name in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, name))]

if __name__ == "__main__":

    for a in subfolders : 
        #print('ğŸˆğŸˆğŸˆ', a)
        folder_path = f"./kia_data/{a}"
        output_file = f"./kia_data_json/{a}.json"    
        #print('ğŸ˜‘', folder_path)
        #print('ğŸ¤–', output_file)               
        parse_pdfs(folder_path, output_file)


# if __name__ == "__main__":
#     folder_path = "./kia_data/ELECTRIFIED,G80(RG3 EV)_136KW+136KW"
#     output_file = "4.json"                    # í•­ìƒ ê°™ì€ ì´ë¦„
#     parse_pdfs(folder_path, output_file)


#--------- ì°¨ type ë³„ json merge ----------------
def merge_json_dicts_to_list(file_list, output_file):
    merged = []
    for f in file_list:
        with open(f, encoding="utf-8") as infile:
            data = json.load(infile)
            if isinstance(data, dict):  # dict êµ¬ì¡°ë§Œ ì¶”ê°€
                merged.append(data)
    with open(output_file, "w", encoding="utf-8") as outfile:
        json.dump(merged, outfile, ensure_ascii=False, indent=2)

# í´ë”ì— ìˆëŠ” ëª¨ë“  JSON í•©ì¹˜ê¸°
files = glob.glob("./hyundai_data_json/*.json")  # kia_data_json
merge_json_dicts_to_list(files, "merged_hyundai.json")


#---------- meta data í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ----------
def parse_json(data_list):
    result = {"ê³ ì¥ì§„ë‹¨": []}

    for data in data_list:   # ë¦¬ìŠ¤íŠ¸ ì•ˆì— ê° ì°¨ì¢… dict
        for model_engine, categories in data.items():   # EV3(SV1)_150kW
            model, engine = model_engine.split("_", 1)

            for category_key, data in categories.items():    # "2026+150KW+ë“œë¼ì´ë¸Œ ìƒ¤í”„íŠ¸ ë° ì•¡ìŠ¬+ê³ ì¥ì§„ë‹¨"
                # "ê³ ì¥ì§„ë‹¨" ì œê±°
                clean_category = category_key.replace("ê³ ì¥ì§„ë‹¨", "").strip("+ ").replace('+', " ").replace('â€“', " ").strip(' ')

                for state, causes in data.items():
                    state = state.replace('â€“', ' ').strip('â€“ ').replace('â€¢', ' ').strip(' ')
                    entry = {
                        "title": f"{clean_category} {state}".strip(" -"),
                        "content": ", ".join(causes),  # ë¦¬ìŠ¤íŠ¸ â†’ ì½¤ë§ˆ êµ¬ë¶„ ë¬¸ìì—´
                        "type": "hyundai",
                        "ì°¨ì¢…": model,
                        "ì—”ì§„": engine,
                    }
                    result["ê³ ì¥ì§„ë‹¨"].append(entry)
    return result

if __name__ == "__main__":
    # JSON ë¶ˆëŸ¬ì˜¤ê¸°
    with open("merged_hyundai_2.json", "r", encoding="utf-8") as f:
        merged_file = json.load(f)

    parsed = parse_json(merged_file)

    # ê²°ê³¼ ì €ì¥
    with open("parsed_hyundai.json", "w", encoding="utf-8") as f:
        json.dump(parsed, f, ensure_ascii=False, indent=2)

    print("ì €ì¥ ì™„ë£Œ: parsed_hyundai.json")


#----------- hyundai & kia ë©”íƒ€ë°ì´í„° merge -----
# íŒŒì¼ ë‘ê°œ ë¶ˆëŸ¬ì˜¤ê¸°
with open("parsed_hyundai.json", "r", encoding="utf-8") as f1:
    list1 = json.load(f1)   # [ {..}, {..}, ... ]

with open("parsed_kia.json", "r", encoding="utf-8") as f2:
    list2 = json.load(f2)   # [ {..}, {..}, ... ]

# ë‘ ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°
merged = list1['ê³ ì¥ì§„ë‹¨'] + list2['ê³ ì¥ì§„ë‹¨']   # extend() ì¨ë„ ë™ì¼

merged_data = {'ê³ ì¥ì§„ë‹¨' : merged}

# ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥
with open("parsed_data.json", "w", encoding="utf-8") as out:
    json.dump(merged_data, out, ensure_ascii=False, indent=2)

print(f"í•©ì¹œ ê°œìˆ˜: {len(merged_data)}")