import torch, datetime
import os, json, re, unicodedata, glob
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores.faiss import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

#----- ê¸°ì¡´ì„¤ì • -----
# GPU ìƒíƒœ í”„ë¦°íŠ¸
print("PyTorch GPU ì‚¬ìš© ê°€ëŠ¥:", torch.cuda.is_available())
print("GPU ê°œìˆ˜:", torch.cuda.device_count())
# print("í˜„ì¬ GPU ì¸ë±ìŠ¤:", torch.cuda.current_device())
# print("GPU ì´ë¦„:", torch.cuda.get_device_name(torch.cuda.current_device()))

# ë¸”ë¡œê·¸ íŒŒì¼ path 
#file_paths = ['./naver_blog_results_tmp.json', './naver_in_results_tmp.json', './parsed_data.json']
file_paths = ['./parsed_data.json']
base_db_path = './faiss_db'

# ë¶„í• 
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=50
)

# ì„ë² ë”© ëª¨ë¸ ì •ì˜ 
embedding_model = HuggingFaceEmbeddings(model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko", model_kwargs={'device':'cpu'}, encode_kwargs={'normalize_embeddings':True})
# embedding_model = HuggingFaceEmbeddings(model_name="Alibaba-NLP/gte-Qwen2-1.5B-instruct", model_kwargs={'device':'cuda'}, encode_kwargs={'normalize_embeddings':True})

# í…ìŠ¤íŠ¸ ì •ë¦¬ 
def clean_string(s: str) -> str:
    if not s:
        return ""
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (í˜¸í™˜ ë¬¸ì í†µí•©)
    s = unicodedata.normalize("NFKC", str(s))
    # ì•ŒíŒŒë²³, í•œê¸€, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¸°ê¸°
    s = re.sub(r"[^0-9a-zA-Zê°€-í£\s]+", " ", s)
    s = re.sub(r"\s+", " ", s)   # ì—¬ëŸ¬ ê³µë°± â†’ í•˜ë‚˜ì˜ ê³µë°±
    return s.strip()             # ì•ë’¤ ê³µë°± ì œê±°


# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° 
for file_path in file_paths :
    with open(file_path, 'r', encoding='utf-8-sig') as f:
        data = json.load(f) # ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë”•ì…”ë„ˆë¦¬ë“¤
    total_len = len(data)
    mini_batch = total_len // 4
    print('!!!!!!!', type(data))
    print('!!!!!!!', data[:3])

    # for val in data :
    # 50ê°œ ë‹¨ìœ„ë¡œ ë£¨í”„ ì½”ë“œ
    #for start_idx in range(0, len(data), 50):
        #batch = data[start_idx:start_idx+50]
        #documents = []

    for i in range(0, mini_batch*1): # ì²«ì„¸íŠ¸ ëŒë¦´ë•Œì˜ ê²½ìš°
        val = data[i]
        documents = []               
        # for keys, vals in item.items():
        #     for val in vals :
        print(f'í˜„ì¬ {i} ë²ˆì§¸ ì²˜ë¦¬ì¤‘~~~~~', datetime.datetime.now())

        # for val in vals :
        if val :
                # ì¼ê´„ ì „ì²˜ë¦¬ 
                val['content'] = clean_string(val['content'])
                val['title'] = clean_string(val['title'])

                # ë¸”ë¡œê·¸
                if val['type'] == 'ë¸”ë¡œê·¸' :
                    splits = splitter.split_text(val['content'])     # ë¦¬ìŠ¤íŠ¸ ì•ˆì— ìª¼ê°œì§„ ë¬¸ìì—´ë“¤ - ë¸”ë¡œê·¸ë§Œ ìª¼ê°œê¸° 

                    for split in splits:
                        document = Document(   # ë¬¸ì„œ ì •ì˜
                            page_content=split,
                            metadata={
                                "title": val['title'],
                                "type":  val['type'],
                                "url":  val['ì¶œì²˜'],
                                "car_type":  val['ì°¨ì¢…'],
                                "engine_type": val['ì—”ì§„'],
                            }
                        )
                        documents.append(document)
                        #print('ğŸˆğŸˆğŸˆë¸”ë¡œê·¸:', document)

                # ì§€ì‹ì¸
                elif val['type'] == 'ì§€ì‹ì¸' :
                    if type(val['content']) == list :
                        answers = {clean_string(answer.get('text','')) for i, answer in enumerate(val['content'])}
                        contents = val['title'] + ' ' + answers
                    else : 
                        contents = val['title'] + ' ' + val['content']
                    
                    splits = splitter.split_text(contents)

                    for split in splits:
                        document = Document(   # ë¬¸ì„œ ì •ì˜
                            page_content=split,
                            metadata={
                                "title": val['title'],
                                "type":  val['type'],
                                "url":  val['ì¶œì²˜'],
                                "car_type":  val['ì°¨ì¢…'],
                                "engine_type": val['ì—”ì§„'],
                            }
                        )
                        documents.append(document)
                        #print('ğŸˆğŸˆğŸˆì§€ì‹ì¸:', document)

                elif val['type'] in ['hyundai','kia']:

                    contents = val['title'] + ' ' + val['content']
                    
                    if contents:
                        document = Document(   # ë¬¸ì„œ ì •ì˜
                            page_content=contents,
                            metadata={
                                "title": val['title'],
                                "type":  val['type'],
                                "url":  val.get('ì¶œì²˜', ''),   # ë©”ë‰´ì–¼ì— ì¶œì²˜ê°€ ì—†ì–´ì„œ ê³µë°± ë°˜í™˜.. Noneìœ¼ë¡œ í•´ì•¼í•˜ë‚˜?? 
                                "car_type":  val['ì°¨ì¢…'],
                                "engine_type": val['ì—”ì§„'],
                            }
                        )
                        documents.append(document)    
                        #print('ğŸˆğŸˆğŸˆë©”ë‰´ì–¼:', document)


                file_name = os.path.splitext(os.path.basename(file_path))[0]
                db_path = os.path.join(base_db_path, file_name)
#
                #if documents :
                #    vector_store = FAISS.from_documents(documents, embedding_model)  # ë²¡í„°í™” 
                #    vector_store.save_local(db_path)
                #    print(f"í˜„ì¬ ì²˜ë¦¬ ì¤‘: {file_path} â†’ DB ì €ì¥ ê²½ë¡œ: {db_path}")
                #else :
                #     print(f'ë¬¸ì„œì—†ìŒ: {file_path}')
#
                #print(f'{file_name} íŒŒì¼ì˜ {1}ë²ˆì§¸ ~ {mini_batch+1}ë²ˆì§¸ ì™„ë£Œ')


                # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ í›„ ì—…ë°ì´íŠ¸
                if os.path.exists(db_path):
                    print("ê¸°ì¡´ DB ë¡œë“œ ì¤‘:", db_path)
                    vector_store = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
                    new_store = FAISS.from_documents(documents, embedding_model)
                    vector_store.merge_from(new_store)  # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ìƒˆ ë°ì´í„° í•©ì¹˜ê¸°
                else:
                    print("ìƒˆ DB ìƒì„±:", db_path)
                    vector_store = FAISS.from_documents(documents, embedding_model)

                # ì €ì¥
                vector_store.save_local(db_path)
                print(f"{file_name} íŒŒì¼ì˜ {1}ë²ˆì§¸ ~ {mini_batch*1}ë²ˆì§¸ ë²¡í„°í™” ì™„ë£Œ â†’ DB ì €ì¥ ê²½ë¡œ: {db_path}")